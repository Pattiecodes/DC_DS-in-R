{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl9/08XItUeMhMYLjJEZMq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_DS-in-R/blob/main/Module_1_Intermediate_Importing_Data_in_R_R.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --- Module Start ---"
      ],
      "metadata": {
        "id": "kHUVCMqIpDYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establish a connection\n",
        "The first step to import data from a SQL database is creating a connection to it. As Filip explained, you need different packages depending on the database you want to connect to. All of these packages do this in a uniform way, as specified in the DBI package.\n",
        "\n",
        "dbConnect()creates a connection between your R session and a SQL database. The first argument has to be a DBIdriver object, that specifies how connections are made and how data is mapped between R and the database. Specifically for MySQL databases, you can build such a driver with RMySQL::MySQL().\n",
        "\n",
        "If the MySQL database is a remote database hosted on a server, you'll also have to specify the following arguments in dbConnect(): dbname, host, port, user and password. Most of these details have already been provided.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the DBI library, which is already installed on DataCamp's servers.\n",
        "Edit the dbConnect() call to connect to the MySQL database. Change the port argument (3306) and user argument (\"student\")."
      ],
      "metadata": {
        "id": "TXir1cz4pHw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the DBI package\n",
        "library(DBI)\n",
        "\n",
        "# Edit dbConnect() call\n",
        "con <- dbConnect(RMySQL::MySQL(),\n",
        "                 dbname = \"tweater\",\n",
        "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
        "                 port = 3306,\n",
        "                 user = \"student\",\n",
        "                 password = \"datacamp\")"
      ],
      "metadata": {
        "id": "KcUfFPuYpJgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import users\n",
        "As you might have guessed by now, the database contains data on a more tasty version of Twitter, namely Tweater. Users can post tweats with short recipes for delicious snacks. People can comment on these tweats. There are three tables: users, tweats, and comments that have relations among them. Which ones, you ask? You'll discover in a moment!\n",
        "\n",
        "Let's start by importing the data on the users into your R session. You do this with the dbReadTable() function. Simply pass it the connection object (con), followed by the name of the table you want to import. The resulting object is a standard R data frame.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Add code that imports the \"users\" table from the tweater database and store the resulting data frame as users.\n",
        "Print the users data frame."
      ],
      "metadata": {
        "id": "uxhpfCgvr0go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the DBI package\n",
        "library(DBI)\n",
        "\n",
        "# Connect to the MySQL database: con\n",
        "con <- dbConnect(RMySQL::MySQL(),\n",
        "                 dbname = \"tweater\",\n",
        "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
        "                 port = 3306,\n",
        "                 user = \"student\",\n",
        "                 password = \"datacamp\")\n",
        "\n",
        "# Import the users table from tweater: users\n",
        "users <- dbReadTable(con, \"users\")\n",
        "\n",
        "# Print users\n",
        "users"
      ],
      "metadata": {
        "id": "YkS0NSfor1dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import all tables\n",
        "Next to the users, we're also interested in the tweats and comments tables. However, separate dbReadTable() calls for each and every one of the tables in your database would mean a lot of code duplication. Remember about the lapply() function? You can use it again here! A connection is already coded for you, as well as a vector table_names, containing the names of all the tables in the database.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Finish the lapply() function to import the users, tweats and comments tables in a single call. The result, a list of data frames, will be stored in the variable tables.\n",
        "Print tables to check if you got it right."
      ],
      "metadata": {
        "id": "hy0pMrZ8sQBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the DBI package\n",
        "library(DBI)\n",
        "\n",
        "# Connect to the MySQL database: con\n",
        "con <- dbConnect(RMySQL::MySQL(),\n",
        "                 dbname = \"tweater\",\n",
        "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
        "                 port = 3306,\n",
        "                 user = \"student\",\n",
        "                 password = \"datacamp\")\n",
        "\n",
        "# Get table names\n",
        "table_names <- dbListTables(con)\n",
        "\n",
        "# Import all tables\n",
        "tables <- lapply(table_names, dbReadTable, conn = con)\n",
        "\n",
        "# Print out tables\n",
        "tables"
      ],
      "metadata": {
        "id": "drMGoDbjsQSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query tweater (1)\n",
        "In your life as a data scientist, you'll often be working with huge databases that contain tables with millions of rows. If you want to do some analyses on this data, it's possible that you only need a fraction of this data. In this case, it's a good idea to send SQL queries to your database, and only import the data you actually need into R.\n",
        "\n",
        "dbGetQuery() is what you need. As usual, you first pass the connection object to it. The second argument is an SQL query in the form of a character string. This example selects the age variable from the people dataset where gender equals \"male\":\n",
        "\n",
        "dbGetQuery(con, \"SELECT age FROM people WHERE gender = 'male'\")\n",
        "A connection to the tweater database has already been coded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use dbGetQuery() to create a data frame, elisabeth, that selects the tweat_id column from the comments table where elisabeth is the commenter, her user_id is 1\n",
        "Print out elisabeth so you can see if you queried the database correctly."
      ],
      "metadata": {
        "id": "Afgn7XsYQqCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the database\n",
        "library(DBI)\n",
        "con <- dbConnect(RMySQL::MySQL(),\n",
        "                 dbname = \"tweater\",\n",
        "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
        "                 port = 3306,\n",
        "                 user = \"student\",\n",
        "                 password = \"datacamp\")\n",
        "\n",
        "# Import tweat_id column of comments where user_id is 1: elisabeth\n",
        "elisabeth <- dbGetQuery(con, \"SELECT tweat_id FROM comments WHERE user_id = 1\")\n",
        "\n",
        "# Print elisabeth\n",
        "print(elisabeth)"
      ],
      "metadata": {
        "id": "wuKLXaR8QqQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query tweater (2)\n",
        "Apart from checking equality, you can also check for less than and greater than relationships, with < and >, just like in R.\n",
        "\n",
        "con, a connection to the tweater database, is again available.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a data frame, latest, that selects the post column from the tweats table observations where the date is higher than '2015-09-21'.\n",
        "Print out latest."
      ],
      "metadata": {
        "id": "2XJxYwPwR0uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the database\n",
        "library(DBI)\n",
        "con <- dbConnect(RMySQL::MySQL(),\n",
        "                 dbname = \"tweater\",\n",
        "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
        "                 port = 3306,\n",
        "                 user = \"student\",\n",
        "                 password = \"datacamp\")\n",
        "\n",
        "# Import post column of tweats where date is higher than '2015-09-21': latest\n",
        "latest <- dbGetQuery(con, \"SELECT post FROM tweats WHERE date > '2015-09-21'\")\n",
        "\n",
        "# Print latest\n",
        "print(latest)"
      ],
      "metadata": {
        "id": "Mn2bk3YwR1Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query tweater (3)\n",
        "Suppose that you have a people table, with a bunch of information. This time, you want to find out the age and country of married males. Provided that there is a married column that's 1 when the person in question is married, the following query would work.\n",
        "```\n",
        "SELECT age, country\n",
        "  FROM people\n",
        "    WHERE gender = \"male\" AND married = 1\n",
        "Can you use a similar approach for a more specialized query on the tweater database?\n",
        "```\n",
        "Instructions\n",
        "100 XP\n",
        "Create an R data frame, specific, that selects the message column from the comments table where the tweat_id is 77 and the user_id is greater than 4.\n",
        "Print specific."
      ],
      "metadata": {
        "id": "rhPgCgaOhMxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the database\n",
        "library(DBI)\n",
        "con <- dbConnect(RMySQL::MySQL(),\n",
        "                 dbname = \"tweater\",\n",
        "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
        "                 port = 3306,\n",
        "                 user = \"student\",\n",
        "                 password = \"datacamp\")\n",
        "\n",
        "# Create data frame specific\n",
        "specific <- dbGetQuery(con, \"SELECT message FROM comments\n",
        "                                WHERE tweat_id=77 AND user_id>4\")\n",
        "\n",
        "# Print specific\n",
        "print(specific)"
      ],
      "metadata": {
        "id": "-ulCCFEohOf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query tweater (4)\n",
        "There are also dedicated SQL functions that you can use in the WHERE clause of an SQL query. For example, CHAR_LENGTH() returns the number of characters in a string.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a data frame, short, that selects the id and name columns from the users table where the number of characters in the name is strictly less than 5.\n",
        "Print short."
      ],
      "metadata": {
        "id": "MrZyplJ2h1Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the database\n",
        "library(DBI)\n",
        "con <- dbConnect(RMySQL::MySQL(),\n",
        "                 dbname = \"tweater\",\n",
        "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
        "                 port = 3306,\n",
        "                 user = \"student\",\n",
        "                 password = \"datacamp\")\n",
        "\n",
        "# Create data frame short\n",
        "short <- dbGetQuery(con, \"SELECT id, name FROM users\n",
        "                            WHERE CHAR_LENGTH(name) < 5\")\n",
        "\n",
        "# Print short\n",
        "print(short)"
      ],
      "metadata": {
        "id": "zBAovypdh1w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Send - Fetch - Clear\n",
        "You've used dbGetQuery() multiple times now. This is a virtual function from the DBI package, but is actually implemented by the RMySQL package. Behind the scenes, the following steps are performed:\n",
        "\n",
        "Sending the specified query with dbSendQuery();\n",
        "Fetching the result of executing the query on the database with dbFetch();\n",
        "Clearing the result with dbClearResult().\n",
        "Let's not use dbGetQuery() this time and implement the steps above. This is tedious to write, but it gives you the ability to fetch the query's result in chunks rather than all at once. You can do this by specifying the n argument inside dbFetch().\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Inspect the dbSendQuery() call that has already been coded for you. It selects the comments for the users with an id above 4.\n",
        "Use dbFetch() twice. In the first call, import only two records of the query result by setting the n argument to 2. In the second call, import all remaining queries (don't specify n). In both calls, simply print the resulting data frames.\n",
        "Clear res with dbClearResult()."
      ],
      "metadata": {
        "id": "xSrLRfZciz_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the database\n",
        "library(DBI)\n",
        "con <- dbConnect(RMySQL::MySQL(),\n",
        "                 dbname = \"tweater\",\n",
        "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
        "                 port = 3306,\n",
        "                 user = \"student\",\n",
        "                 password = \"datacamp\")\n",
        "\n",
        "# Send query to the database\n",
        "res <- dbSendQuery(con, \"SELECT * FROM comments WHERE user_id > 4\")\n",
        "\n",
        "# Use dbFetch() twice\n",
        "dbFetch(res, n=2)\n",
        "dbFetch(res)\n",
        "\n",
        "# Clear res\n",
        "dbClearResult(res)"
      ],
      "metadata": {
        "id": "OwDHzEMpi0VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Be polite and ...\n",
        "Every time you connect to a database using dbConnect(), you're creating a new connection to the database you're referencing. RMySQL automatically specifies a maximum of open connections and closes some of the connections for you, but still: it's always polite to manually disconnect from the database afterwards. You do this with the dbDisconnect() function.\n",
        "\n",
        "The code that connects you to the database is already available, can you finish the script?\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Using the technique you prefer, build a data frame long_tweats. It selects the post and date columns from the observations in tweats where the character length of the post variable exceeds 40.\n",
        "Print long_tweats.\n",
        "Disconnect from the database by using dbDisconnect()."
      ],
      "metadata": {
        "id": "rzWW-olVji-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the database\n",
        "library(DBI)\n",
        "con <- dbConnect(RMySQL::MySQL(),\n",
        "                 dbname = \"tweater\",\n",
        "                 host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\",\n",
        "                 port = 3306,\n",
        "                 user = \"student\",\n",
        "                 password = \"datacamp\")\n",
        "\n",
        "# Create the data frame  long_tweats\n",
        "long_tweats <- dbGetQuery(con, \"SELECT post, date FROM tweats\n",
        "                                                WHERE CHAR_LENGTH(post) > 40\")\n",
        "\n",
        "# Print long_tweats\n",
        "print(long_tweats)\n",
        "\n",
        "# Disconnect from the database\n",
        "dbDisconnect(con)"
      ],
      "metadata": {
        "id": "h3uaH95hjjSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import flat files from the web\n",
        "In the video, you saw that the utils functions to import flat file data, such as read.csv() and read.delim(), are capable of automatically importing from URLs that point to flat files on the web.\n",
        "\n",
        "You must be wondering whether Hadley Wickham's alternative package, readr, is equally potent. Well, figure it out in this exercise! The URLs for both a .csv file as well as a .delim file are already coded for you. It's up to you to actually import the data. If it works, that is…\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the readr package. It's already installed on DataCamp's servers.\n",
        "Use url_csv to read in the .csv file it is pointing to. Use the read_csv() function. The .csv contains column names in the first row. Save the resulting data frame as pools.\n",
        "Similarly, use url_delim to read in the online .txt file. Use the read_tsv() function and store the result as potatoes.\n",
        "Print pools and potatoes. Looks correct?"
      ],
      "metadata": {
        "id": "eyP6IDpDy39K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the readr package\n",
        "library(readr)\n",
        "\n",
        "# Import the csv file: pools\n",
        "url_csv <- \"http://assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv\"\n",
        "pools <- read.csv(url_csv)\n",
        "\n",
        "# Import the txt file: potatoes\n",
        "url_delim <- \"http://assets.datacamp.com/production/course_1478/datasets/potatoes.txt\"\n",
        "potatoes <- read.delim(url_delim)\n",
        "\n",
        "# Print pools and potatoes\n",
        "print(pools)\n",
        "print(potatoes)"
      ],
      "metadata": {
        "id": "0DU6wjY0z8OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Secure importing\n",
        "In the previous exercises, you have been working with URLs that all start with http://. There is, however, a safer alternative to HTTP, namely HTTPS, which stands for HyperText Transfer Protocol Secure. Just remember this: HTTPS is relatively safe, HTTP is not.\n",
        "\n",
        "Luckily for us, you can use the standard importing functions with https:// connections since R version 3.2.2.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Take a look at the URL in url_csv. It uses a secure connection, https://.\n",
        "Use read.csv() to import the file at url_csv. The .csv file it is referring to contains column names in the first row. Call it pools1.\n",
        "Load the readr package. It's already installed on DataCamp's servers.\n",
        "Use read_csv() to read in the same .csv file in url_csv. Call it pools2.\n",
        "Print out the structure of pools1 and pools2. Looks like the importing went equally well as with a normal http connection!"
      ],
      "metadata": {
        "id": "I83X8f10zh4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the readr package\n",
        "library(readr)\n",
        "\n",
        "# Import the csv file: pools\n",
        "url_csv <- \"http://assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv\"\n",
        "pools <- read.csv(url_csv)\n",
        "\n",
        "# Import the txt file: potatoes\n",
        "url_delim <- \"http://assets.datacamp.com/production/course_1478/datasets/potatoes.txt\"\n",
        "potatoes <- read.delim(url_delim)\n",
        "\n",
        "# Print pools and potatoes\n",
        "print(pools)\n",
        "print(potatoes)"
      ],
      "metadata": {
        "id": "oS9ouYa0ziN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Excel files from the web\n",
        "When working with multiple data sources, .xls is a common format for storage and exchange. You have been provided with a URL which you'll be using to download and import a .xls file. The URL is already available in the sample code. Once downloaded, you will import the file using readxl and inspect its content.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the readxl package. It is already installed on DataCamp's servers.\n",
        "Use download.file() to download the .xls file behind the URL and store it locally as \"local_latitude.xls\".\n",
        "Import the .xls file located at the URL url_xls using read_excel(). Store the resulting data frame as excel_readxl."
      ],
      "metadata": {
        "id": "BGetL5iSyY-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the readxl package\n",
        "library(readxl)\n",
        "\n",
        "# Specification of url: url_xls\n",
        "url_xls <- \"https://assets.datacamp.com/production/course_1478/datasets/latitude.xls\"\n",
        "\n",
        "# Download file behind URL, name it local_latitude.xls\n",
        "download.file(url_xls, destfile = \"local_latitude.xls\")\n",
        "\n",
        "# Import the local .xls file\n",
        "excel_readxl <- read_excel(\"local_latitude.xls\")"
      ],
      "metadata": {
        "id": "ujl3XVo_yZXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading any file, secure or not\n",
        "In the previous exercise you've seen how you can read excel files on the web using the read_excel package by first downloading the file with the download.file() function.\n",
        "\n",
        "There's more: with download.file() you can download any kind of file from the web, using HTTP and HTTPS: images, executable files, but also .RData files. An RData file is very efficient format to store R data.\n",
        "\n",
        "You can load data from an RData file using the load() function, but this function does not accept a URL string as an argument. In this exercise, you'll first download the RData file securely, and then import the local data file.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Take a look at the URL in url_rdata. It uses a secure connection, https://. This URL points to an RData file containing a data frame with some metrics on different kinds of wine.\n",
        "Download the file at url_rdata using download.file(). Call the file \"wine_local.RData\" in your working directory.\n",
        "Load the file you created, wine_local.RData, using the load() function. It takes one argument, the path to the file, which is just the filename in our case. After running this command, the variable wine will automatically be available in your workspace.\n",
        "Print out the summary() of the wine dataset."
      ],
      "metadata": {
        "id": "gB3dcTQrzzGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https URL to the wine RData file.\n",
        "url_rdata <- \"https://assets.datacamp.com/production/course_1478/datasets/wine.RData\"\n",
        "\n",
        "# Download the wine file to your working directory\n",
        "download.file(url_rdata, destfile = \"wine_local.RData\")\n",
        "\n",
        "# Load the wine data into your workspace using load()\n",
        "load(\"wine_local.RData\")\n",
        "\n",
        "# Print out the summary of the wine data\n",
        "summary(wine)"
      ],
      "metadata": {
        "id": "7luDVRxKzzWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HTTP? httr! (1)\n",
        "Downloading a file from the Internet means sending a GET request and receiving the file you asked for. Internally, all the previously discussed functions use a GET request to download files.\n",
        "\n",
        "httr provides a convenient function, GET() to execute this GET request. The result is a response object, that provides easy access to the status code, content-type and, of course, the actual content.\n",
        "\n",
        "You can extract the content from the request using the content() function. At the time of writing, there are three ways to retrieve this content: as a raw object, as a character vector, or an R object, such as a list. If you don't tell content() how to retrieve the content through the as argument, it'll try its best to figure out which type is most appropriate based on the content-type.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the httr package. It's already installed on DataCamp's servers.\n",
        "Use GET() to get the URL stored in url. Store the result of this GET() call as resp.\n",
        "Print the resp object. What information does it contain?\n",
        "Get the content of resp using content() and set the as argument to \"raw\". Assign the resulting vector to raw_content.\n",
        "Print the first values in raw_content with head()."
      ],
      "metadata": {
        "id": "MGrfBpsLKB_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the httr package\n",
        "library(httr)\n",
        "\n",
        "# Get the url, save response to resp\n",
        "url <- \"http://www.example.com/\"\n",
        "resp <- GET(url)\n",
        "\n",
        "# Print resp\n",
        "resp\n",
        "\n",
        "# Get the raw content of resp: raw_content\n",
        "raw_content <- content(resp, as = \"raw\")\n",
        "\n",
        "# Print the head of raw_content\n",
        "head(raw_content)"
      ],
      "metadata": {
        "id": "KhIrxeCgKCjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HTTP? httr! (2)\n",
        "Web content does not limit itself to HTML pages and files stored on remote servers such as DataCamp's Amazon S3 instances. There are many other data formats out there. A very common one is JSON. This format is very often used by so-called Web APIs, interfaces to web servers with which you as a client can communicate to get or store information in more complicated ways.\n",
        "\n",
        "You'll learn about Web APIs and JSON in the video and exercises that follow, but some experimentation never hurts, does it?\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use GET() to get the url that has already been specified in the sample code. Store the response as resp.\n",
        "Print resp. What is the content-type?\n",
        "Use content() to get the content of resp. Set the as argument to \"text\". Simply print out the result. What do you see?\n",
        "Use content() to get the content of resp, but this time do not specify a second argument. R figures out automatically that you're dealing with a JSON, and converts the JSON to a named R list."
      ],
      "metadata": {
        "id": "wfOs1N8mK7SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# httr is already loaded\n",
        "\n",
        "\n",
        "# Get the url\n",
        "url <- \"http://www.omdbapi.com/?apikey=72bc447a&t=Annie+Hall&y=&plot=short&r=json\"\n",
        "resp <- GET(url)\n",
        "\n",
        "# Print resp\n",
        "resp\n",
        "\n",
        "# Print content of resp as text\n",
        "content(resp, as=\"text\")\n",
        "\n",
        "# Print content of resp\n",
        "content(resp)"
      ],
      "metadata": {
        "id": "iCPD3Ih-K7jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From JSON to R\n",
        "In the simplest setting, fromJSON() can convert character strings that represent JSON data into a nicely structured R list. Give it a try!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the jsonlite package. It's already installed on DataCamp's servers.\n",
        "wine_json represents a JSON. Use fromJSON() to convert it to a list, named wine.\n",
        "Display the structure of wine"
      ],
      "metadata": {
        "id": "u2hXAorYLxEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the jsonlite package\n",
        "library(jsonlite)\n",
        "\n",
        "# wine_json is a JSON\n",
        "wine_json <- '{\"name\":\"Chateau Migraine\", \"year\":1997, \"alcohol_pct\":12.4, \"color\":\"red\", \"awarded\":false}'\n",
        "\n",
        "# Convert wine_json into a list: wine\n",
        "wine <- fromJSON(wine_json)\n",
        "\n",
        "# Print structure of wine\n",
        "str(wine)"
      ],
      "metadata": {
        "id": "B-0RakpgLxYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import JSON data from API\n",
        "As Filip showed in the video, fromJSON() also works if you pass a URL as a character string or the path to a local file that contains JSON data. Let's try this out on the OMDb API, where you can fetch all sorts of movie data.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "omdb_url represents a URL. Use fromJSON() directly on this URL and store the result in omdb_url.\n",
        "Display the structure of omdb_url."
      ],
      "metadata": {
        "id": "6SM_EpavMPAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jsonlite is preloaded\n",
        "\n",
        "# Definition of omdb_url\n",
        "omdb_url <- \"http://www.omdbapi.com/?apikey=72bc447a&i=tt0076759&r=json\"\n",
        "\n",
        "# Import OMDb data: omdb_data\n",
        "omdb_data <- fromJSON(omdb_url)\n",
        "\n",
        "# Print structure of omdb_data\n",
        "str(omdb_data)"
      ],
      "metadata": {
        "id": "DZZQQo7HMPST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OMDb API\n",
        "In the video, you saw how easy it is to interact with an API once you know how to formulate requests. You also saw how to fetch all information on Rain Man from OMDb. Simply perform a GET() call, and next ask for the contents with the content() function. This content() function, which is part of the httr package, uses jsonlite behind the scenes to import the JSON data into R.\n",
        "\n",
        "However, by now you also know that jsonlite can handle URLs itself. Simply passing the request URL to fromJSON() will get your data into R. In this exercise, you will be using this technique to compare the release year of two movies in the Open Movie Database.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Two URLs are included in the sample code, as well as a fromJSON() call to build sw4. Add a similar call to build sw3.\n",
        "Print out the element named Title of both sw4 and sw3. You can use the $ operator. What movies are we dealing with here?\n",
        "Write an expression that evaluates to TRUE if sw4 was released later than sw3. This information is stored in the Year element of the named lists."
      ],
      "metadata": {
        "id": "vqu_PiYvT9ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The package jsonlite is already loaded\n",
        "\n",
        "# Definition of the URLs\n",
        "url_sw4 <- \"http://www.omdbapi.com/?apikey=72bc447a&i=tt0076759&r=json\"\n",
        "url_sw3 <- \"http://www.omdbapi.com/?apikey=72bc447a&i=tt0121766&r=json\"\n",
        "\n",
        "# Import two URLs with fromJSON(): sw4 and sw3\n",
        "sw4 <- fromJSON(url_sw4)\n",
        "sw3 <- fromJSON(url_sw3)\n",
        "\n",
        "# Print out the Title element of both lists\n",
        "print(sw4$Title)\n",
        "print(sw3$Title)\n",
        "\n",
        "# Is the release year of sw4 later than sw3?\n",
        "sw4$Year > sw3$Year"
      ],
      "metadata": {
        "id": "N0iFGp_bT9ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JSON practice (1)\n",
        "JSON is built on two structures: objects and arrays. To help you experiment with these, two JSON strings are included in the sample code. It's up to you to change them appropriately and then call jsonlite's fromJSON() function on them each time.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Change the assignment of json1 such that the R vector after conversion contains the numbers 1 up to 6, in ascending order. Next, call fromJSON() on json1.\n",
        "Adapt the code for json2 such that it's converted to a named list with two elements: a, containing the numbers 1, 2 and 3 and b, containing the numbers 4, 5 and 6. Next, call fromJSON() on json2."
      ],
      "metadata": {
        "id": "KxtT1dsSZWp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jsonlite is already loaded\n",
        "\n",
        "# Challenge 1\n",
        "json1 <- '[1, 2, 3, 4, 5, 6]'\n",
        "fromJSON(json1)\n",
        "\n",
        "# Challenge 2\n",
        "json2 <- '{\"a\": [1, 2, 3], \"b\": [4, 5, 6]}'\n",
        "fromJSON(json2)\n"
      ],
      "metadata": {
        "id": "HvAyzkFVZW61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JSON practice (2)\n",
        "We prepared two more JSON strings in the sample code. Can you change them and call jsonlite's fromJSON() function on them, similar to the previous exercise?\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Remove characters from json1 to build a 2 by 2 matrix containing only 1, 2, 3 and 4. Call fromJSON() on json1.\n",
        "Add characters to json2 such that the data frame in which the json is converted contains an additional observation in the last row. For this observations, a equals 5 and b equals 6. Call fromJSON() one last time, on json2."
      ],
      "metadata": {
        "id": "VFmzUrSkwbaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jsonlite is already loaded\n",
        "\n",
        "# Challenge 1\n",
        "json1 <- '[[1, 2], [3, 4]]'\n",
        "fromJSON(json1)\n",
        "\n",
        "# Challenge 2\n",
        "json2 <- '[{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}, {\"a\": 5, \"b\": 6}]'\n",
        "fromJSON(json2)"
      ],
      "metadata": {
        "id": "3T-cc07Awb2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# toJSON()\n",
        "Apart from converting JSON to R with fromJSON(), you can also use toJSON() to convert R data to a JSON format. In its most basic use, you simply pass this function an R object to convert to a JSON. The result is an R object of the class json, which is basically a character string representing that JSON.\n",
        "\n",
        "For this exercise, you will be working with a .csv file containing information on the amount of desalinated water that is produced around the world. As you'll see, it contains a lot of missing values. This data can be found on the URL that is specified in the sample code.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use a function of the utils package to import the .csv file directly from the URL specified in url_csv. Save the resulting data frame as water.\n",
        "Convert the data frame water to a JSON. Call the resulting object water_json.\n",
        "Print out water_json.\n",
        "\n"
      ],
      "metadata": {
        "id": "6dZPcZ4kykuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jsonlite is already loaded\n",
        "\n",
        "# URL pointing to the .csv file\n",
        "url_csv <- \"https://assets.datacamp.com/production/course_1478/datasets/water.csv\"\n",
        "\n",
        "# Import the .csv file located at url_csv\n",
        "water <- read.csv(url_csv)\n",
        "\n",
        "# Convert the data file according to the requirements\n",
        "water_json <- toJSON(water)\n",
        "\n",
        "# Print out water_json\n",
        "print(water_json)"
      ],
      "metadata": {
        "id": "npchvv5UylE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minify and prettify\n",
        "JSONs can come in different formats. Take these two JSONs, that are in fact exactly the same: the first one is in a minified format, the second one is in a pretty format with indentation, whitespace and new lines:\n",
        "```\n",
        "# Mini\n",
        "{\"a\":1,\"b\":2,\"c\":{\"x\":5,\"y\":6}}\n",
        "\n",
        "# Pretty\n",
        "{\n",
        "  \"a\": 1,\n",
        "  \"b\": 2,\n",
        "  \"c\": {\n",
        "    \"x\": 5,\n",
        "    \"y\": 6\n",
        "  }\n",
        "}\n",
        "```\n",
        "Unless you're a computer, you surely prefer the second version. However, the standard form that toJSON() returns, is the minified version, as it is more concise. You can adapt this behavior by setting the pretty argument inside toJSON() to TRUE. If you already have a JSON string, you can use prettify() or minify() to make the JSON pretty or as concise as possible.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Convert the mtcars dataset, which is available in R by default, to a pretty JSON. Call the resulting JSON pretty_json.\n",
        "Print out pretty_json. Can you understand the output easily?\n",
        "Convert pretty_json to a minimal version using minify(). Store this version under a new variable, mini_json.\n",
        "Print out mini_json. Which version do you prefer, the pretty one or the minified one?"
      ],
      "metadata": {
        "id": "O-FtpT37zgvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jsonlite is already loaded\n",
        "\n",
        "# Convert mtcars to a pretty JSON: pretty_json\n",
        "pretty_json <- toJSON(mtcars, pretty = TRUE)\n",
        "\n",
        "# Print pretty_json\n",
        "print(pretty_json)\n",
        "\n",
        "# Minify pretty_json: mini_json\n",
        "mini_json <- minify(pretty_json)\n",
        "\n",
        "# Print mini_json\n",
        "print(mini_json)"
      ],
      "metadata": {
        "id": "QgZVPeTMzjLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import SAS data with haven\n",
        "haven is an extremely easy-to-use package to import data from three software packages: SAS, STATA and SPSS. Depending on the software, you use different functions:\n",
        "\n",
        "SAS: read_sas()\n",
        "STATA: read_dta() (or read_stata(), which are identical)\n",
        "SPSS: read_sav() or read_por(), depending on the file type.\n",
        "All these functions take one key argument: the path to your local file. In fact, you can even pass a URL; haven will then automatically download the file for you before importing it.\n",
        "\n",
        "You'll be working with data on the age, gender, income, and purchase level (0 = low, 1 = high) of 36 individuals (Source: SAS). The information is stored in a SAS file, sales.sas7bdat, which is available in your current working directory. You can also download the data here.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the haven package; it's already installed on DataCamp's servers.\n",
        "Import the data file \"sales.sas7bdat\". Call the imported data frame sales.\n",
        "Display the structure of sales with str(). Some columns represent categorical variables, so they should be factors."
      ],
      "metadata": {
        "id": "dCNdvliSSb6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the haven package\n",
        "library(haven)\n",
        "\n",
        "# Import sales.sas7bdat: sales\n",
        "sales <- read_sas(\"sales.sas7bdat\")\n",
        "\n",
        "# Display the structure of sales\n",
        "str(sales)"
      ],
      "metadata": {
        "id": "oClIF75pScPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import STATA data with haven\n",
        "Next up are STATA data files; you can use read_dta() for these.\n",
        "\n",
        "When inspecting the result of the read_dta() call, you will notice that one column will be imported as a labelled vector, an R equivalent for the common data structure in other statistical environments. In order to effectively continue working on the data in R, it's best to change this data into a standard R class. To convert a variable of the class labelled to a factor, you'll need haven's as_factor() function.\n",
        "\n",
        "In this exercise, you will work with data on yearly import and export numbers of sugar, both in USD and in weight. The data can be found at: https://assets.datacamp.com/production/course_1478/datasets/trade.dta\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import the data file directly from the URL using read_dta(), and store it as sugar.\n",
        "Print out the structure of sugar. The Date column has class labelled.\n",
        "Convert the values in the Date column of sugar to dates, using as.Date(as_factor(___)).\n",
        "Print out the structure of sugar once more. Looks better now?"
      ],
      "metadata": {
        "id": "ZDHmnCytSo1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# haven is already loaded\n",
        "\n",
        "# Import the data from the URL: sugar\n",
        "sugar <- read_dta(\"https://assets.datacamp.com/production/course_1478/datasets/trade.dta\")\n",
        "\n",
        "# Structure of sugar\n",
        "str(sugar)\n",
        "\n",
        "# Convert values in Date column to dates\n",
        "sugar$Date <- as.Date(as_factor(sugar$Date))\n",
        "\n",
        "# Structure of sugar again\n",
        "str(sugar)"
      ],
      "metadata": {
        "id": "wq6tI1ZUSpD2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}