{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN04ffz23ZklXIrhcLdi2qI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_DS-in-R/blob/main/Module_4_Feature_Engineering_in_R_R.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---\n",
        "\n"
      ],
      "metadata": {
        "id": "IbR-0h-jhcgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A tentative model\n",
        "You are handed a data set with measures of the gravitational force between two bodies at different distances and are challenged to build a simple model to predict such force given a specific distance. Initially, you want to stick to simple linear regression. The data consist of 120 pairs of distance and force, and is loaded for you as newton.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Build a linear model for the newton data using the linear model from base R function and assign it to lr_force.\n",
        "Create a new data frame df by binding the prediction values to the original newton data.\n",
        "Generate a scatterplot of force versus distance using ggplot().\n",
        "Add a regression line to the scatterplot with the fitted values."
      ],
      "metadata": {
        "id": "AenshZBrhfWw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-H6c9M3NtH1"
      },
      "outputs": [],
      "source": [
        "# Build a linear model for the newton the data and assign it to lr_force\n",
        "lr_force <- lm(force ~ distance, data = newton)\n",
        "\n",
        "# Create a new data frame by binding the prediction values to the original data\n",
        "df <- newton %>% bind_cols(lr_pred = predict(lr_force))\n",
        "\n",
        "# Generate a scatterplot of force vs. distance\n",
        "df %>%\n",
        "  ggplot(aes(x = distance, y = force)) +\n",
        "  geom_point() +\n",
        "# Add a regression line with the fitted values\n",
        "  geom_line(aes(y = lr_pred), color = \"blue\", lwd = .75) +\n",
        "  ggtitle(\"Linear regression of force vs. distance\") +\n",
        "  theme_classic()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manually engineering a feature\n",
        "After doing some research with your team, you recall that the gravitational force of attraction between two bodies obeys Newton's formula:\n",
        "\n",
        "\n",
        "$$\n",
        "F = G \\frac{m_1 m_2}{r^2}\n",
        "$$.\n",
        "\n",
        "You can't use the formula directly because the masses are unknown, but you can fit a regression model of force as a function of inv_square_distance. The augmented dataset df you built in the previous exercise has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a new variable inv_square_distance defined as the reciprocal of the squared distance and add it to the df data frame.\n",
        "Build a simple regression model using lm() of force versus inv_square_distance and save it as lr_force_2.\n",
        "Bind your predictions to df_inverse."
      ],
      "metadata": {
        "id": "lqcgdxddk5Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new variable inv_square_distance\n",
        "df_inverse <- df %>% mutate(inv_square_distance = 1/distance^2)\n",
        "\n",
        "# Build a simple regression model\n",
        "lr_force_2 <- lm(force ~ inv_square_distance, data = df_inverse)\n",
        "\n",
        "# Bind your predictions to df_inverse\n",
        "df_inverse <- df_inverse %>% bind_cols(lr2_pred = predict(lr_force_2))\n",
        "\n",
        "df_inverse %>% ggplot(aes(x = distance, y = force)) +\n",
        "  geom_point() +\n",
        "  geom_line(aes(y = lr2_pred), col = \"blue\", lwd = .75) +\n",
        "  ggtitle(\"Linear regression of force vs. inv_square_distance\") +\n",
        "  theme_classic()"
      ],
      "metadata": {
        "id": "RQcWKKqplkkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up your data for analysis\n",
        "You will look at a version of the nycflights13 dataset, loaded as flights. It contains information on flights departing from New York City. You are interested in predicting whether or not they will arrive late to their destination, but first, you need to set up the data for analysis.\n",
        "\n",
        "After discussing our model goals with a team of experts, you selected the following variables for your model: flight, sched_dep_time, dep_delay, sched_arr_time, carrier, origin, dest, distance, date, arrival.\n",
        "\n",
        "You will also mutate() the date using as.Date() and convert character type variables to factors.\n",
        "\n",
        "Lastly, you will split the data into train and test datasets.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Transform all character-type variables to factors.\n",
        "Split the flights data into test and train sets."
      ],
      "metadata": {
        "id": "wkSM9aNz9NDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flights <- flights %>%\n",
        "  select(flight, sched_dep_time, dep_delay, sched_arr_time, carrier, origin, dest, distance, date, arrival) %>%\n",
        "\n",
        "# Tranform all character-type variables to factors\n",
        "  mutate(date = as.Date(date), across(where(is.character), as.factor))\n",
        "\n",
        "# Split the flights data into test and train sets\n",
        "set.seed(246)\n",
        "split <- flights %>% initial_split(prop = 3/4, strata = arrival)\n",
        "test <- testing(split)\n",
        "train <- training(split)\n",
        "\n",
        "test %>% select(arrival) %>% table() %>% prop.table()\n",
        "train %>% select(arrival) %>% table() %>% prop.table()"
      ],
      "metadata": {
        "id": "TFjviP9u9Ntc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a workflow\n",
        "With your data ready for analysis, you will declare a logistic_model() to predict whether or not they will arrive late.\n",
        "\n",
        "You assign the role of \"ID\" to the flight variable to keep it as a reference for analysis and debugging. From the date variable, you will create new features to explicitly model the effect of holidays and represent factors as dummy variables.\n",
        "\n",
        "Bundling your model and recipe() together using workflow()will help ensure that subsequent fittings or predictions will implement consistent feature engineering steps.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Assign an \"ID\" role to flight.\n",
        "Bundle the model and the recipe into a workflow object.\n",
        "Fit lr_workflow to the test data.\n",
        "Tidy the fitted workflow."
      ],
      "metadata": {
        "id": "IBzXbsU7-FvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model <- logistic_reg()\n",
        "\n",
        "# Assign an \"ID\" role to flight\n",
        "lr_recipe <- recipe(arrival ~., data = train) %>% update_role(flight, new_role = \"ID\") %>%\n",
        "  step_holiday(date, holidays = timeDate::listHolidays(\"US\")) %>% step_dummy(all_nominal_predictors())\n",
        "\n",
        "# Bundle the model and the recipe into a workflow object\n",
        "lr_workflow <- workflow() %>% add_model(lr_model) %>% add_recipe(lr_recipe)\n",
        "lr_workflow\n",
        "\n",
        "# Fit lr_workflow workflow to the test data\n",
        "lr_fit <- lr_workflow %>% fit(data = test)\n",
        "\n",
        "# Tidy the fitted workflow\n",
        "tidy(lr_fit)"
      ],
      "metadata": {
        "id": "GJqmQrKZ-GEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifying missing values\n",
        "Attrition is a critical issue for corporations, as losing an employee implies not only the cost of recruiting and training a new one, but constitutes a loss in tacit knowledge and culture that is hard to recover.\n",
        "\n",
        "The attritiondataset has information on employee attrition including Age, WorkLifeBalance, DistanceFromHome, StockOptionLevel, and 27 others. Before continuing with your analysis, you want to detect any missing variables.\n",
        "\n",
        "The package naniar and the attritiondataset are already loaded for you."
      ],
      "metadata": {
        "id": "naUdrdzN4r9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore missing data on the attrition dataset\n",
        "vis_miss(attrition)"
      ],
      "metadata": {
        "id": "FAhkTBOa4tAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "Select the variables with missing values and visualize only those."
      ],
      "metadata": {
        "id": "qMAVIht-4uqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the variables with missing values and rerun the analysis on those variables.\n",
        "attrition %>%\n",
        "  select(\"BusinessTravel\", \"DistanceFromHome\",\n",
        "         \"StockOptionLevel\", \"WorkLifeBalance\") %>%\n",
        "  vis_miss()"
      ],
      "metadata": {
        "id": "lC56zlJY4xNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imputing missing values and creating dummy variables\n",
        "After detecting missing values in the attrition dataset and determining that they are missing completely at random (MCAR), you decide to use K Nearest Neighbors (KNN) imputation. While configuring your feature engineering recipe, you decide to create dummy variables for all your nominal variables and update the role of the ...1 variable to \"ID\" so you can keep it in the dataset for reference, without affecting your model.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Update the role of ...1 to \"ID\".\n",
        "Impute values to all predictors where data are missing.\n",
        "Create dummy variables for all nominal predictors."
      ],
      "metadata": {
        "id": "nDnBdTkQ5pjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model <- logistic_reg()\n",
        "\n",
        "lr_recipe <-\n",
        "  recipe(Attrition ~., data = train) %>%\n",
        "\n",
        "# Update the role of \"...1\" to \"ID\"\n",
        "  update_role(...1, new_role = \"ID\" ) %>%\n",
        "\n",
        "# Impute values to all predictors where data are missing\n",
        "  step_impute_knn(all_predictors()) %>%\n",
        "\n",
        "# Create dummy variables for all nominal predictors\n",
        "  step_dummy(all_nominal_predictors())\n",
        "\n",
        "lr_recipe"
      ],
      "metadata": {
        "id": "AdjHL0Dd5qIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting and assessing the model\n",
        "Now that you have addressed missing values and created dummy variables, it is time to assess your model's performance!\n",
        "\n",
        "The attritiondataset, along with the testand train splits, the lr_recipe and your declared logistic_model() are all loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Bundle model and recipe in workflow.\n",
        "Fit workflow to the train data.\n",
        "Generate an augmented data frame for performance assessment."
      ],
      "metadata": {
        "id": "JeYINv8R_wIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bundle model and recipe in workflow\n",
        "lr_workflow <- workflow() %>%\n",
        "  add_model(lr_model) %>%\n",
        "  add_recipe(lr_recipe)\n",
        "\n",
        "# Fit workflow to the train data\n",
        "lr_fit <- fit(lr_workflow, data = train)\n",
        "\n",
        "# Generate an augmented data frame for performance assessment\n",
        "lr_aug <- lr_fit %>% augment(test)\n",
        "\n",
        "lr_aug %>% roc_curve(truth = Attrition, .pred_No) %>% autoplot()\n",
        "bind_rows(lr_aug %>% roc_auc(truth = Attrition, .pred_No),\n",
        "          lr_aug %>% accuracy(truth = Attrition, .pred_class))"
      ],
      "metadata": {
        "id": "4Mzmm0QQ_wcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting hotel bookings\n",
        "You just got a job at a hospitality research company, and your first task is to build a model that predicts whether or not a hotel stay will include children. To train your model, you will rely on a modified version of the hotel booking demands dataset by Antonio, Almeida, and Nunes (2019). You are restricting your data to the following features:\n",
        "```\n",
        "features <- c('hotel', 'adults',\n",
        "              'children', 'meal',\n",
        "              'reserved_room_type',\n",
        "              'customer_type',\n",
        "              'arrival_date')\n",
        "```\n",
        "The data has been loaded for you as hotels, along with its corresponding test and train splits, and the model has been declared as lr_model <- logistic_reg().\n",
        "\n",
        "You will assess model performance by accuracy and area under the ROC curve or AUC.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Generate \"day of the week\", \"week\" and \"month\" features.\n",
        "Create dummy variables for all nominal predictors."
      ],
      "metadata": {
        "id": "rGtfpm4rAkW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_recipe <-\n",
        "  recipe(children ~., data = train) %>%\n",
        "# Generate \"day of the week\", \"week\" and \"month\" features\n",
        "\n",
        "  step_date(arrival_date, features = c(\"dow\", \"week\", \"month\")) %>%\n",
        "\n",
        "# Create dummy variables for all nominal predictors\n",
        "  step_dummy(all_nominal_predictors())"
      ],
      "metadata": {
        "id": "Fhfz0v-UAkrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Bundle your model and recipe in a workflow().\n",
        "Fit the workflow to the training data."
      ],
      "metadata": {
        "id": "8-D12-L0Axdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_recipe <- recipe(children ~., data = train) %>%\n",
        "\n",
        "# Generate \"day of the week\", \"week\" and \"month\" features\n",
        "  step_date(arrival_date, features = c(\"dow\", \"week\", \"month\")) %>%\n",
        "\n",
        "# Create dummy variables for all nominal predictors\n",
        "  step_dummy(all_nominal_predictors())\n",
        "\n",
        "# Bundle your model and recipe in a workflow\n",
        "lr_workflow <-workflow() %>% add_model(lr_model) %>% add_recipe(lr_recipe)\n",
        "\n",
        "# Fit the workflow to the training data\n",
        "lr_fit <-  lr_workflow %>% fit(data = train)\n",
        "lr_aug <- lr_fit %>% augment(test)\n",
        "bind_rows(roc_auc(lr_aug,truth = children, .pred_children),accuracy(lr_aug,truth = children, .pred_class))"
      ],
      "metadata": {
        "id": "Y_UuUyJZAxtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizing and log-transforming\n",
        "You are handed a dataset, attrition_num with numerical data about employees who left the company. Features include Age, DistanceFromHome, and MonthlyRate.\n",
        "\n",
        "You want to use this data to build a model that can predict if an employee is likely to stay, denoted by Attrition, a binary variable coded as a factor. In preparation for modeling, you want to reduce possible skewness and prevent some variables from outweighing others due to variations in scale.\n",
        "\n",
        "The attrition_numdata and the trainand test splits are loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Normalize all numeric predictors.\n",
        "Log-transform all numeric features, with an offset of one.\n",
        "\n",
        "Take Hint (-30 XP)"
      ],
      "metadata": {
        "id": "PN5qeLZNKZFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model <- logistic_reg()\n",
        "\n",
        "lr_recipe <-\n",
        "  recipe(Attrition~., data = train) %>%\n",
        "\n",
        "# Normalize all numeric predictors\n",
        "  step_normalize(all_numeric_predictors()) %>%\n",
        "\n",
        "# Log-transform all numeric features, with an offset of one\n",
        "  step_log(all_numeric_predictors(), offset = 1)\n",
        "\n",
        "lr_workflow <-\n",
        "  workflow() %>%\n",
        "  add_model(lr_model) %>%\n",
        "  add_recipe(lr_recipe)\n",
        "\n",
        "lr_workflow"
      ],
      "metadata": {
        "id": "BKT42k-3KZef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit and augment\n",
        "With your lr_workflow ready to go, you can fit it to the test data to make predictions.\n",
        "\n",
        "For model assessment, it is convenient to augment your fitted object by adding predictions and probabilities using augment().\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Fit the workflow to the train data.\n",
        "Augment the lr_fit object using the test data to get it ready for assessment."
      ],
      "metadata": {
        "id": "s09z0WKeL06z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the workflow to the train data\n",
        "lr_fit <-\n",
        "  fit(lr_workflow, data = train)\n",
        "\n",
        "# Augment the lr_fit object\n",
        "lr_aug <-\n",
        "  augment(lr_fit, new_data = test)\n",
        "\n",
        "lr_aug"
      ],
      "metadata": {
        "id": "N6dKeP-XL1XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customize your model assessment\n",
        "Creating custom assessment functions is quite convenient when iterating through various models. The metric_set() function from the yardstickpackage can help you to achieve this.\n",
        "\n",
        "Define a function that returns roc_auc, accuracy, sens(sensitivity) and specificity spec(specificity) and use it to assess your model.\n",
        "\n",
        "The augmented data frame lr_augis already loaded and ready to go.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define a custom assessment function that returns roc_auc, accuracy, sens, and spec.\n",
        "Assess your model using your new function on lr_augto obtain the metrics you just chose."
      ],
      "metadata": {
        "id": "_Ig8afoLMpWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom assessment function\n",
        "class_evaluate <- metric_set(roc_auc, accuracy, sens, spec)\n",
        "\n",
        "# Assess your model using your new function\n",
        "class_evaluate(lr_aug, truth = Attrition,\n",
        "               estimate = .pred_class,\n",
        "               .pred_No)"
      ],
      "metadata": {
        "id": "jlKAEF0kMp0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plain recipe\n",
        "Using the attrition_num dataset with all numerical data about employees who have left the company, you want to build a model that can predict if an employee is likely to stay, using Attrition, a binary variable coded as a factor. To get started, you will define a plain recipe that does nothing other than define the model formula and the training data.\n",
        "\n",
        "The attrition_numdata, the logistic regression lr_model, the user-defined class-evaluate() function, and the trainand test splits are loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a plain recipe defining only the model formula."
      ],
      "metadata": {
        "id": "PGJSZEhk0rbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a plain recipe defining only the model formula\n",
        "lr_recipe_plain <-\n",
        "  recipe(Attrition ~., data = train)\n",
        "\n",
        "lr_workflow_plain <- workflow() %>%\n",
        "  add_model(lr_model) %>%\n",
        "  add_recipe(lr_recipe_plain)\n",
        "lr_fit_plain <- lr_workflow_plain %>%\n",
        "  fit(train)\n",
        "lr_aug_plain <-\n",
        "  lr_fit_plain %>% augment(test)\n",
        "lr_aug_plain %>% class_evaluate(truth = Attrition,\n",
        "                 estimate = .pred_class,.pred_No)"
      ],
      "metadata": {
        "id": "CHDSr7Fg0rre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Box-Cox transformation\n",
        "Using the attrition_num dataset with all numerical data about employees who have left the company, you want to build a model that can predict if an employee is likely to stay, using Attrition, a binary variable coded as a factor. To get the features to behave nearly normally, you will create a recipe that implements the Box-Cox transformation.\n",
        "\n",
        "The attrition_numdata, the logistic regression lr_model, the user-defined class-evaluate() function, and the trainand test splits are loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a recipe that uses Box-Cox to transform all numeric features, including the target."
      ],
      "metadata": {
        "id": "9u4hnMaI075F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a recipe that uses Box-Cox to transform all numeric features\n",
        "lr_recipe_BC <-\n",
        "  recipe(Attrition ~., data = train) %>%\n",
        "  step_BoxCox(all_numeric())\n",
        "\n",
        "lr_workflow_BC <- workflow() %>%\n",
        "  add_model(lr_model) %>%\n",
        "  add_recipe(lr_recipe_BC)\n",
        "lr_fit_BC <- lr_workflow_BC %>%\n",
        "  fit(train)\n",
        "lr_aug_BC <-\n",
        "  lr_fit_BC %>% augment(test)\n",
        "lr_aug_BC %>% class_evaluate(truth = Attrition,\n",
        "                 estimate = .pred_class,.pred_No)"
      ],
      "metadata": {
        "id": "2bCrcS0u08KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yeo-Johnson transformation\n",
        "Using the attrition_num dataset with all numerical data about employees who have left the company, you want to build a model that can predict if an employee is likely to stay, using Attrition, a binary variable coded as a factor. To get the features to behave nearly normally, you will create a recipe that implements the Yeo-Johnson transformation.\n",
        "\n",
        "The attrition_numdata, the logistic regression lr_model, the user-defined class-evaluate() function, and the trainand test splits are loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a recipe that uses Yeo-Johnson to transform all numeric features, including the target."
      ],
      "metadata": {
        "id": "ydXGtadyi1DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a recipe that uses Yeo-Johnson to transform all numeric features\n",
        "lr_recipe_YJ <-\n",
        "  recipe(Attrition ~., data = train) %>%\n",
        "  step_YeoJohnson(all_numeric())\n",
        "\n",
        "lr_workflow_YJ <- workflow() %>%\n",
        "  add_model(lr_model) %>%\n",
        "  add_recipe(lr_recipe_YJ)\n",
        "lr_fit_YJ <- lr_workflow_YJ %>%\n",
        "  fit(train)\n",
        "lr_aug_YJ <-\n",
        "  lr_fit_YJ %>% augment(test)\n",
        "lr_aug_YJ %>% class_evaluate(truth = Attrition,\n",
        "                 estimate = .pred_class,.pred_No)"
      ],
      "metadata": {
        "id": "qZBk5ksgi1XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline\n",
        "Continuing with the attrition_num dataset, you will create a baseline with a plain recipe to assess the effects of additional feature engineering steps. The attrition_numdata, the logistic regression lr_model, the user-defined class-evaluate() function, and the trainand test splits have already been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Bundle the model and recipe into a workflow.\n",
        "Augment the fitted workflow to get it ready for assessment."
      ],
      "metadata": {
        "id": "Tf9eedsjm-cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_recipe_plain <- recipe(Attrition ~., data = train)\n",
        "\n",
        "# Bundle the model and recipe\n",
        "lr_workflow_plain <- workflow() %>%\n",
        "  add_model(lr_model) %>%\n",
        "  add_recipe(lr_recipe_plain)\n",
        "lr_fit_plain <- lr_workflow_plain %>%\n",
        "  fit(train)\n",
        "\n",
        "# Augment the fit workflow\n",
        "lr_aug_plain <- lr_fit_plain %>%\n",
        "  augment(test)\n",
        "lr_aug_plain %>%\n",
        "  class_evaluate(truth = Attrition,estimate = .pred_class,\n",
        "                 .pred_No)"
      ],
      "metadata": {
        "id": "IGtaf3kHm-uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step_poly()\n",
        "Now that you have a baseline, you can compare your model's performance if you add a polynomial transformation to all numerical values.\n",
        "\n",
        "The attrition_numdata, the logistic regression lr_model, the user-defined class-evaluate() function, and the trainand test splits have already been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Add a polynomial transformation to all numeric predictors.\n",
        "Fit workflow to the train data."
      ],
      "metadata": {
        "id": "zewRXDnibxTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_recipe_poly <-\n",
        "  recipe(Attrition ~., data = train) %>%\n",
        "\n",
        "# Add a polynomial transformation to all numeric predictors\n",
        "  step_poly(all_numeric_predictors())\n",
        "\n",
        "lr_workflow_poly <- workflow() %>%\n",
        "  add_model(lr_model) %>%\n",
        "  add_recipe(lr_recipe_poly)\n",
        "\n",
        "# Fit workflow to the train data\n",
        "lr_fit_poly <- lr_workflow_poly %>% fit(train)\n",
        "lr_aug_poly <- lr_fit_poly %>% augment(test)\n",
        "lr_aug_poly %>% class_evaluate(truth = Attrition, estimate = .pred_class,.pred_No)"
      ],
      "metadata": {
        "id": "loSeJo63bxrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step_percentile()\n",
        "How would applying a percentile transformation to your numeric variables affect model performance? Try it!\n",
        "\n",
        "The attrition_numdata, the logistic regression lr_model, the user-defined class-evaluate() function, and the trainand test splits have already been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Apply a percentile transformation to all numeric predictors."
      ],
      "metadata": {
        "id": "rUjQRD1PcDWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add percentile tansformation to all numeric predictors\n",
        "lr_recipe_perc <-\n",
        "  recipe(Attrition ~., data = train) %>%\n",
        "  step_percentile(all_numeric_predictors())\n",
        "lr_workflow_perc <-\n",
        "  workflow() %>%\n",
        "  add_model(lr_model) %>%\n",
        "  add_recipe(lr_recipe_perc)\n",
        "lr_fit_perc <- lr_workflow_perc %>% fit(train)\n",
        "lr_aug_perc <- lr_fit_perc %>% augment(test)\n",
        "lr_aug_perc %>% class_evaluate(truth = Attrition,\n",
        "                 estimate = .pred_class,.pred_No)"
      ],
      "metadata": {
        "id": "wQ2dNYmhcDrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Who's staying?\n",
        "It's time to practice combining several transformations to the attrition_num data. First, normalize or near-normalize numeric variables by applying a Yeo-Johnson transformation. Next, transform numeric predictors to percentiles, create dummy variables, and eliminate features with near zero variance.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Apply a Yeo-Johnson transformation to all numeric variables.\n",
        "Transform all numeric predictors into percentiles.\n",
        "Create dummy variables for all nominal predictors."
      ],
      "metadata": {
        "id": "UAdvOp03ctv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_recipe <- recipe(Attrition ~., data = train) %>%\n",
        "\n",
        "# Apply a Yeo-Johnson transformation to all numeric variables\n",
        "  step_YeoJohnson(all_numeric()) %>%\n",
        "\n",
        "# Transform all numeric predictors into percentiles\n",
        "  step_percentile(all_numeric_predictors()) %>%\n",
        "\n",
        "# Create dummy variables for all nominal predictors\n",
        "  step_dummy(all_nominal_predictors())\n",
        "\n",
        "lr_workflow <- workflow() %>% add_model(lr_model) %>% add_recipe(lr_recipe)\n",
        "lr_fit <- lr_workflow %>% fit(train)\n",
        "lr_aug <- lr_fit %>% augment(test)\n",
        "lr_aug %>% class_evaluate(truth = Attrition, estimate = .pred_class,.pred_No)"
      ],
      "metadata": {
        "id": "XuPsIU9DcuCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepping the stage\n",
        "You are going to explore the attrition_num dataset from the point of view of PCA to understand if it is feasible to reduce dimensionality while preserving most information. Start by creating a recipe that filters our near-zero variance features, normalizes the data, and implements PCA.\n",
        "\n",
        "The attrition_num dataset is already loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Remove possible near-zero variance features.\n",
        "Normalize all numeric data.\n",
        "Apply PCA.\n",
        "Access the names of the output elements by preparing the recipe."
      ],
      "metadata": {
        "id": "zHGi8gX1-7d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pc_recipe <- recipe(~., data = attrition_num) %>%\n",
        "\n",
        "# Remove possible near-zero variance features\n",
        "  step_nzv(all_numeric()) %>%\n",
        "\n",
        "# Normalize all numeric data\n",
        "  step_normalize(all_numeric()) %>%\n",
        "\n",
        "# Apply PCA\n",
        "  step_pca(all_numeric())\n",
        "\n",
        "# Access the names of the output elements by preparing the recipe\n",
        "pca_output <- names(pc_recipe)\n",
        "names(pca_output)"
      ],
      "metadata": {
        "id": "yBimA56e-78m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Percent of variance explained\n",
        "From the pca_output, you can retrieve the standard deviation explained by each principal component. Then, use these values to compute the variance explained and the cumulative variance explained, and glue together these values into a tibble.\n",
        "\n",
        "The pca_output object is loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Calculate percentage of variance explained leveraging the standard deviation vector.\n",
        "Create a tibble with principal components, variance explained and cumulative variance explained."
      ],
      "metadata": {
        "id": "y3NuIWRS_yZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdev <- pca_output$steps[[3]]$res$sdev\n",
        "# Calculate percentage of variance explained\n",
        "var_explained <- sdev^2 / sum(sdev^2)\n",
        "\n",
        "# Create a tibble with principal components, variance explained and cumulative variance explained\n",
        "PCA = tibble(PC = 1:length(sdev), var_explained = var_explained,\n",
        "       cumulative = cumsum(var_explained))"
      ],
      "metadata": {
        "id": "Bsh3N8rp_yxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing variance explained\n",
        "With all the calculations under your belt, it always comes in handy to represent our data visually. You will now create a column plot showing variance explained by principal component.\n",
        "\n",
        "The variable_explained vector you created in the last exercise is available, and the ggplot() theme_() is set to classic.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use the information in the PCA tibble to create a column plot of variance explained."
      ],
      "metadata": {
        "id": "eGP85XbL2oEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PCA = tibble(PC = 1:length(sdev), var_explained = var_explained,\n",
        "       cumulative = cumsum(var_explained))\n",
        "\n",
        "# Use the information in the PCA tibble to create a column plot of variance explained\n",
        "PCA %>% ggplot(aes(x = PC, y = var_explained)) +\n",
        "  geom_col(fill = \"steelblue\") +\n",
        "  xlab(\"Principal components\") +\n",
        "  ylab(\"Variance explained\")"
      ],
      "metadata": {
        "id": "gRzHQmAG2obT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Investigating education field\n",
        "Education field is a feature of the attrition dataset. You are interested in using it as a predictor for churning. Start by taking a look at the factor values. The dataset is already loaded.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Select EducationField from the attrition dataset.\n",
        "Print a frequency table of the factor values in EducationField."
      ],
      "metadata": {
        "id": "dOLebt6z85lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attrition %>%\n",
        "# Select education field\n",
        "  select(EducationField) %>%\n",
        "\n",
        "# Print a frequency table of factor values\n",
        "  table()"
      ],
      "metadata": {
        "id": "DNSvEQFP853u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Into the matrix\n",
        "You identified six distinct values for EducationField. But you suspect that others might show up as you run the model on new data. To prepare for this, you will create a hash index with 50 terms. The textrecipes package, the attrition_train, and the attrition_test splits are already loaded.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Add a step to the recipe that generates a dummy_hash index for EducationField.\n",
        "Prepare the recipe.\n",
        "Bake the prepared recipe.\n",
        "Bind the baked recipe table and the EducationField values into one table and print the first 7 rows, as well as columns 1 and 18 to 20."
      ],
      "metadata": {
        "id": "oLnkl5C0GXpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recipe <- recipe(~EducationField, data = attrition_train) %>%\n",
        "# Add a step to the recipe that generates a dummy_hash index for EducationField\n",
        "  step_dummy_hash(EducationField, prefix = NULL, signed = FALSE, num_terms = 50L)\n",
        "\n",
        "# Prepare the recipe\n",
        "object <- recipe %>%\n",
        "  prep()\n",
        "\n",
        "# Bake the prepped recipe\n",
        "baked <- bake(object, new_data = attrition_test)\n",
        "\n",
        "# Bind the baked recipe table and the EducationField values into one table\n",
        "bind_cols(attrition_test$EducationField, baked)[1:7,c(1,18:20)]"
      ],
      "metadata": {
        "id": "OknMP5gxGX8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the hashing\n",
        "It is often helpful to look at a visual representation of your data. The baked tibble is loaded with the hash indexes representing the EducationField factor. You can explore all or a portion of this dataset as a matrix to identify patterns or detect potential errors.\n",
        "\n",
        "The plot.matrix package has already been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Convert the baked tibble to a matrix.\n",
        "Plot the attrition_hash matrix."
      ],
      "metadata": {
        "id": "VYj0ehuZHwUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the baked tibble to a matrix\n",
        "attrition_hash <- as.matrix(baked)[1:50,]\n",
        "\n",
        "# Plot the attrTheTition_hash matrix\n",
        "plot(attrition_hash,\n",
        "     col = c(\"white\",\"steelblue\"),\n",
        "     key = NULL,\n",
        "     border = NA)"
      ],
      "metadata": {
        "id": "-zM29tC5Hwod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up your workflow\n",
        "You want to investigate if JobRole alone can be a predictor for Attrition. Given that JobRole is a factor, you plan to use a Bayes encoder to represent it numerically in your model.\n",
        "\n",
        "The embed package and the corresponding test and train splits from the attrition dataset are loaded in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create recipe using the Bayes encoder.\n",
        "Bundle the model and recipe with a workflow."
      ],
      "metadata": {
        "id": "QfrR-0vKTSo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model <- logistic_reg()\n",
        "# Create recipe using the Bayes encoder\n",
        "lr_recipe_glm <-\n",
        "  recipe(Attrition ~ JobRole, data = train) %>%\n",
        "  step_lencode_bayes(JobRole, outcome = vars(Attrition))\n",
        "\n",
        "# Bundle with workflow\n",
        "lr_workflow_glm <-\n",
        "  workflow() %>%\n",
        "  add_model(lr_model) %>%\n",
        "  add_recipe(lr_recipe_glm)\n",
        "\n",
        "lr_workflow_glm\n"
      ],
      "metadata": {
        "id": "F4sK5EABTS3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting, augmenting, and assessing\n",
        "You are ready to fit the workflow to the training data and assess its performance using the testing split.\n",
        "\n",
        "The embed package, the lr_workflow_glm object, and the corresponding test and train splits are in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Fit the workflow to the training set.\n",
        "Augment the fit using the test split.\n",
        "Assess the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "MhmZ0qSoTwKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the workflow to the training set\n",
        "lr_fit_glm <- lr_workflow_glm %>%\n",
        "  fit(train)\n",
        "\n",
        "# Augment the fit using the test split\n",
        "lr_aug_glm <- lr_fit_glm %>%\n",
        "  augment(test)\n",
        "\n",
        "# Assess the model\n",
        "glm_metrics <- lr_aug_glm %>%\n",
        "  class_evaluate(truth = Attrition,\n",
        "                 estimate = .pred_class,\n",
        "                 .pred_Yes)\n",
        "\n",
        "glm_metrics"
      ],
      "metadata": {
        "id": "0hDkXditTwbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binding models together\n",
        "When running several models, it is useful to summarize your results for comparison, both as a tibble and as a parallel coordinates chart. The glm_metrics object that your created is loaded in the workspace, as well as the corresponding bayes_metrics and mixed_metrics. The GGally package is ready for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Bind all three models by rows.\n",
        "Create a parallel coordinates plot to compare the models' performance in both metrics."
      ],
      "metadata": {
        "id": "mJwL1YhLHq0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model <- c(\"glm\", \"glm\",\"bayes\",\"bayes\", \"mixed\", \"mixed\")\n",
        "\n",
        "# Bind models by rows\n",
        "models <- bind_rows(glm_metrics,bayes_metrics, mixed_metrics)%>%\n",
        "  add_column(model = model)%>%\n",
        "  select(-.estimator) %>%\n",
        "  spread(model,.estimate)\n",
        "\n",
        "models\n",
        "\n",
        "# Create a parallel coordinates plot\n",
        "ggparcoord(models,\n",
        "           columns = 2:4, groupColumn = 1,\n",
        "           scale=\"globalminmax\",\n",
        "           showPoints = TRUE)"
      ],
      "metadata": {
        "id": "ydmEEiTNHrVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a workflow\n",
        "As you keep investigating attrition, it is natural to build a model that takes all the available predictors, hopping to get a highly accurate prediction. Let's see how it goes.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a recipe to predict Attrition based on all features.\n",
        "Bundle the model and recipe in a workflow."
      ],
      "metadata": {
        "id": "asgSV-w-OZZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model <- logistic_reg()\n",
        "\n",
        "# Create a recipe to predict Attrition based on all features\n",
        "lr_recipe <-\n",
        "  recipe(Attrition ~.,\n",
        "         data = train)\n",
        "\n",
        "# Bundle the model and recipe in a workflow\n",
        "lr_workflow <-\n",
        "  workflow() %>%\n",
        "  add_model(lr_model) %>%\n",
        "  add_recipe(lr_recipe)\n",
        "\n",
        "lr_workflow"
      ],
      "metadata": {
        "id": "ZUHyRt2-OZtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit and augment\n",
        "Now is time to fit and augment our model. You are using all available variables and have some expectations on the results. Can this relatively larger model outperform simpler ones? Take a look at the lr_augment object to see how your model is performing.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Augment the fit object to assess the model."
      ],
      "metadata": {
        "id": "Txvzl-I-4r49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_fit <- lr_workflow %>%\n",
        "  fit(test)\n",
        "\n",
        "# Augment the fit object to assess the model\n",
        "lr_aug <- lr_fit %>%\n",
        "  augment(test)\n",
        "\n",
        "lr_aug %>% class_evaluate(truth = Attrition,\n",
        "                          estimate = .pred_class,\n",
        "                          .pred_No)\n",
        "lr_aug"
      ],
      "metadata": {
        "id": "c8cwwyJz4sQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Which is the main predictor?\n",
        "Which is the main predictor?\n",
        "You've got a remarkable prediction, but what were the main predictors? How can you make sense of the model so that you can go beyond the raw results? Machine learning models are often criticized for their lack of interpretability. However, variable importance rankings shed some light on the relevance of your chosen features with the outcome. So let's investigate variable importance and go from there.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a variable importance chart."
      ],
      "metadata": {
        "id": "hjYqakxW5HYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_fit <- lr_workflow %>%\n",
        "  fit(test)\n",
        "\n",
        "lr_aug <- lr_fit %>%\n",
        "  augment(test)\n",
        "\n",
        "lr_aug %>% class_evaluate(truth = Attrition,\n",
        "                          estimate = .pred_class,\n",
        "                          .pred_No)\n",
        "\n",
        "# Create a variable importance chart\n",
        "lr_fit %>%\n",
        "  extract_fit_parsnip() %>%\n",
        "  vip(aesthetics = list(fill = \"steelblue\"), num_features = 5)"
      ],
      "metadata": {
        "id": "oqPz2aFN5Htm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sifting through variable importance\n",
        "The attrition dataset contains 839 observations and 30 predictors for \"Attrition.\" You are interested in exploring the trade-off between the performance of a model that uses all available predictors versus a reduced model based on a few informative variables.\n",
        "\n",
        "In this exercise, you'll fit a model and have a look at the variable importance of this fitted model. In the following exercise, you'll assess model performance using this model compared to using a reduced model.\n",
        "\n",
        "The train and test splits and the vip() package are available in your environment along with a predeclared logistic regression model.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a recipe that models Attrition using all predictors.\n",
        "Fit the workflow to the training data.\n",
        "Use the fit_full object to graph the variable importance of your model.\n",
        "Apply the extract_fit_parsnip() function before vip() to feed it the required information."
      ],
      "metadata": {
        "id": "lQyjsDoLcdR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a recipe that models Attrition using all the predictors\n",
        "recipe_full <- recipe(Attrition ~ ., data = train)\n",
        "\n",
        "workflow_full <- workflow() %>%\n",
        "  add_model(model) %>%\n",
        "  add_recipe(recipe_full)\n",
        "\n",
        "# Fit the workflow to the training data\n",
        "fit_full <- workflow_full %>%\n",
        "  fit(data = train)\n",
        "\n",
        "# Use the fit_full object to graph the variable importance of your model. Apply extract_fit_parsnip() function before vip()\n",
        "fit_full %>% extract_fit_parsnip() %>%\n",
        "  vip(aesthetics = list(fill = \"steelblue\"))"
      ],
      "metadata": {
        "id": "x1w616A0cdi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessing model performance using all available predictors\n",
        "In order to assess the performance of your reduced model, it is important to set a benchmark. Measure your full model's performance to understand the trade-off of a reduced model. Recall the variable importance chart that you created in an earlier exercise.\n",
        "\n",
        "The train and test splits together with your user-defined function class_evaluate() are loaded in your environment. Your fitted model has been saved as fit_full. Thetrain and test splits together with your user-defined function class_evaluate() are loaded in your environment.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create an augmented object from the fitted full model.\n",
        "Assess model performance using class_evaluate."
      ],
      "metadata": {
        "id": "STYs0PIHdL9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an augmented object from the fitted full model\n",
        "aug_full <-\n",
        "  fit_full %>%\n",
        "  augment(test)\n",
        "\n",
        "# Assess model performance using class_evaluate\n",
        "aug_full %>% class_evaluate(truth = Attrition,\n",
        "               estimate = .pred_class,\n",
        "               .pred_Yes)"
      ],
      "metadata": {
        "id": "jfmUzZtddPXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a reduced model\n",
        "Variable importance analysis helped you identify the most predictive features from the attrition dataset. Based on it, you will build a drastically reduced model with only three variables: OverTime, DistanceFromHome, and NumCompaniesWorked and compare its performance to the full model baseline. The metrics you estimated for the full model are stored in aug_full.\n",
        "\n",
        "All data, along with the train and test splits, is available in your environment.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Create a recipe using the formula syntax that includes only OverTime and DistanceFromHome as predictors.\n",
        "Bundle the recipe with your model."
      ],
      "metadata": {
        "id": "dkRDYI2XdvQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a recipe using the formula syntax that includes only OverTime, DistanceFromHome and NumCompaniesWorked as predictors\n",
        "recipe_reduced <-\n",
        "  recipe(Attrition ~ OverTime + DistanceFromHome + NumCompaniesWorked, data = train)\n",
        "\n",
        "# Bundle the recipe with your model\n",
        "workflow_reduced <-\n",
        "  workflow() %>%\n",
        "  add_model(model) %>%\n",
        "  add_recipe(recipe_reduced)"
      ],
      "metadata": {
        "id": "dXsWP1cBdvq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "3\n",
        "Augment the fitted workflow for analysis using the test data."
      ],
      "metadata": {
        "id": "DDLMZOeYfVdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a recipe using the formula syntax that includes only OverTime, DistanceFromHome and NumCompaniesWorked as predictors\n",
        "recipe_reduced <-\n",
        "  recipe(Attrition ~ OverTime + DistanceFromHome + NumCompaniesWorked, data = train)\n",
        "\n",
        "# Bundle the recipe with your model\n",
        "workflow_reduced <-\n",
        "  workflow() %>%\n",
        "  add_model(model) %>%\n",
        "  add_recipe(recipe_reduced)\n",
        "\n",
        "fit_reduced <-\n",
        "  workflow_reduced %>%\n",
        "  fit(data = train)\n",
        "\n",
        "# Augment the fitted workflow for analysis using the test data\n",
        "aug_reduced <-\n",
        "  fit_reduced %>%\n",
        "  augment(test)"
      ],
      "metadata": {
        "id": "K1wDI-yJfVot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "0 XP\n",
        "Evaluate your reduced model for comparison."
      ],
      "metadata": {
        "id": "uu9TeKABf1f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a recipe using the formula syntax that includes only OverTime, DistanceFromHome and NumCompaniesWorked as predictors\n",
        "recipe_reduced <-\n",
        "  recipe(Attrition ~ OverTime + DistanceFromHome + NumCompaniesWorked, data = train)\n",
        "\n",
        "# Bundle the recipe with your model\n",
        "workflow_reduced <-\n",
        "  workflow() %>%\n",
        "  add_model(model) %>%\n",
        "  add_recipe(recipe_reduced)\n",
        "\n",
        "fit_reduced <-\n",
        "  workflow_reduced %>%\n",
        "  fit(data = train)\n",
        "\n",
        "# Augment the fitted workflow for analysis using the test data\n",
        "aug_reduced <-\n",
        "  fit_reduced %>%\n",
        "  augment(test)\n",
        "\n",
        "full_model <- aug_full %>% class_evaluate(truth = Attrition,\n",
        "                            estimate = .pred_class, .pred_Yes)\n",
        "\n",
        "# Evaluate your reduced model for comparison\n",
        "reduced_model <- aug_reduced %>% class_evaluate(truth = Attrition,\n",
        "                                             estimate = .pred_class, .pred_Yes)\n",
        "\n",
        "bind_cols(full_model,reduced_model) %>%\n",
        "  select(1,3,6) %>%\n",
        "  rename(metric = .metric...1, full_model = .estimate...3,\n",
        "         reduced_model = .estimate...6)"
      ],
      "metadata": {
        "id": "oBKaPRUXf119"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual regularization with Lasso\n",
        "The attrition dataset has 30 variables. Your Human Resources department asks you to build a model that is easy to interpret and maintain. They specifically want to reduce the number of features so that your model is as interpretable as possible.\n",
        "\n",
        "In this exercise, you'll use Lasso to reduce the number of variables in your model automatically. In this first attempt, you will manually input a penalty and observe the model's behavior.\n",
        "\n",
        "trainand test data, and a basic recipe are already loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Set your logistic regression model to use the glmnet engine.\n",
        "Set arguments to run Lasso with a penalty of 0.06."
      ],
      "metadata": {
        "id": "9Fs7gCqW49vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lasso_manual <- logistic_reg() %>%\n",
        "\n",
        "# Set the glmnet engine for your logistic regression model\n",
        "  set_engine(\"glmnet\") %>%\n",
        "\n",
        "# Set arguments to run Lasso with a penalty of 0.06\n",
        "  set_args(mixture = 1, penalty = 0.06)\n",
        "\n",
        "workflow_lasso_manual <- workflow() %>%\n",
        "  add_model(model_lasso_manual) %>%\n",
        "  add_recipe(recipe)\n",
        "\n",
        "fit_lasso_manual <- workflow_lasso_manual %>%\n",
        "  fit(train)\n",
        "\n",
        "tidy(fit_lasso_manual)"
      ],
      "metadata": {
        "id": "WZTFpQZu4-A1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}